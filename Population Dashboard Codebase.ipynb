{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e68b055-e9f0-4237-8cf4-001ba31659eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This notebook combines three separate scripts written for the live population dashboard project. The first part contains the code for some function defined and used later in the project. It also contains the websites from which data was scraped. The second part contains the main scripts that scrapes the data, cleans the data and writes it into postgress database. The third part contains the script to grab OSM data via Geofabrik and write into a postgresql database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ba7b5-8fde-47ad-9a0f-78a5a2cf896f",
   "metadata": {},
   "source": [
    "**Contributor: Selorm Komla Darkey*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d31ea-1c6a-45ef-a9f5-ef425b9025a0",
   "metadata": {},
   "source": [
    "**Part One:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8d50a3-9151-48da-a01e-822790bb4055",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Defining Connect to the database\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_connection\u001b[39m():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "# Importing library\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Defining Connect to the database\n",
    "def get_connection():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"geodb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"mydbms1\"\n",
    "        )\n",
    "    return conn\n",
    "\n",
    "\n",
    "schema_containing_tables = \"hft\"\n",
    "\n",
    "\n",
    "# DATA BANK FOR ALL STATISTIK WEBSITE LINKS AND TABLE NAMES #\n",
    "# ----------- POPULATION BY NATIONALITY LINKS  ------------------#\n",
    "nat_links = [\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/MigrNation/01035010.tab?R=RB1\",   # Stuttgart Administrative\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/MigrNation/01035010.tab?R=RB2\",   # Karlsruhe Administrative\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/MigrNation/01035010.tab?R=KR116\",  # City District Esslingen\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/MigrNation/01035010.tab?R=KR111\",  # City District of Stuttgart\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/MigrNation/01035010.tab?R=KR212\"    # City District of Karlsruhe\n",
    " ]\n",
    "# ---------- POPULATIONS BY AGE GROUPS LINKS  --------------------#\n",
    "age_group_links = [\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/Alter/01035410.tab?R=RB1\",       # Stuttgart Administrative\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/Alter/01035410.tab?R=RB2\",       # Karlsruhe Administrative\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/Alter/01035410.tab?R=KR116\",     # City District Esslingen\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/Alter/01035410.tab?R=KR111\",     # City District of Stuttgart\n",
    "        \"https://www.statistik-bw.de/BevoelkGebiet/Alter/01035410.tab?R=KR212\"      # City District of Karlsruhe\n",
    " ]\n",
    "# ---------- TABLE NAMES TO BE USED FOR THE CSV  --------------------#\n",
    "table_names = [[\"popbynat_stuttgartadmin\", \"popbyagegroups_stuttgartadmin\"],\n",
    "               [\"popbynat_karlsruheadmin\", \"popbyagegroups_karlsruheadmin\"],\n",
    "               [\"popbynat_esslingencity\", \"popbyagegroups_esslingencity\"],\n",
    "               [\"popbynat_stuttgartcity\", \"popbyagegroups_stuttgartcity\"],\n",
    "               [\"popbynat_karlsruhecity\", \"popbyagegroups_karlsruhecity\"]]\n",
    "\n",
    "lat_lon = [[48.76998, 9.1752525],       # stuttgart\n",
    "           [49.006889, 8.403653],       # karlsruhe\n",
    "           [48.73961, 9.30473],         # esslingen\n",
    "           [48.76998, 9.1752525],       # stuttgart_city\n",
    "           [49.006889, 8.403653]]       # karlsruhe_city\n",
    "\n",
    "special_id = [21, 15, 22, 29, 738]  # Stuttgart_admin, karlsruhe, esslingen, stuttgart_city,\n",
    "\n",
    "\n",
    "def create_table_age_groups(schema, table_name):\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {schema}.{table_name}\n",
    "(\n",
    "        id integer NOT NULL GENERATED ALWAYS AS IDENTITY ( INCREMENT 1 START 1 MINVALUE 1 MAXVALUE 2147483647 CACHE 1 ),\n",
    "        age_years integer,      \n",
    "        total_pop integer,\n",
    "        under15 integer,\n",
    "        fifteenToEighteen integer,\n",
    "        EighteenToTwentyfive integer,\n",
    "        twentyfiveToforty integer,\n",
    "        fortyTosixtyfive integer,\n",
    "        sixtyfiveAndAbove integer,\n",
    "        latitude double precision,\n",
    "        longitude double precision,\n",
    "        index integer,\n",
    "        createdon date DEFAULT CURRENT_DATE,\n",
    "        updatedon date DEFAULT CURRENT_DATE,\n",
    "        CONSTRAINT \"{table_name}_pkey\" PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "        TABLESPACE pg_default;\n",
    "        \"\"\"\n",
    "    cur.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_table_nats(schema, table_name):\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {schema}.{table_name}\n",
    "(\n",
    "        id integer NOT NULL GENERATED ALWAYS AS IDENTITY ( INCREMENT 1 START 1 MINVALUE 1 MAXVALUE 2147483647 CACHE 1 ),\n",
    "        nat_years integer,\n",
    "        total_pop integer,\n",
    "        total_masculine integer,\n",
    "        total_german integer,\n",
    "        german_masculine integer,\n",
    "        total_foreigners integer,\n",
    "        foreigner_masculine integer,\n",
    "        latitude double precision,\n",
    "        longitude double precision,\n",
    "        index integer,\n",
    "        createdon date DEFAULT CURRENT_DATE,\n",
    "        updatedon date DEFAULT CURRENT_DATE,\n",
    "        CONSTRAINT \"{table_name}_pkey\" PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "        TABLESPACE pg_default;\n",
    "        \"\"\"\n",
    "    cur.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# insects data into population tables. takes a list, schema name, table name\n",
    "def insert_data_a(data_to_insert, schema, table_name):\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    insert_sql = f\"\"\"\n",
    "                    INSERT INTO {schema}.\"{table_name}\"(\n",
    "                    age_years, total_pop, under15, fifteenToEighteen, EighteenToTwentyfive, twentyfiveToforty,\n",
    "                    fortyTosixtyfive, sixtyfiveAndAbove, latitude, longitude, index)\n",
    "                    VALUES ({data_to_insert[0]}, {data_to_insert[1]}, {data_to_insert[2]}, {data_to_insert[3]},\n",
    "                            {data_to_insert[4]}, {data_to_insert[5]}, {data_to_insert[6]}, {data_to_insert[7]},\n",
    "                            {data_to_insert[8]}, {data_to_insert[9]}, {data_to_insert[10]});\n",
    "                \"\"\"\n",
    "\n",
    "    cur.execute(insert_sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# SECOND INSECT DATA #\n",
    "def insert_data_n(data_to_insert, schema, table_name):     # for population by nationality\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    insert_sql = f\"\"\"\n",
    "                    INSERT INTO {schema}.{table_name}(\n",
    "                    nat_years, total_pop, total_masculine, total_german, german_masculine, total_foreigners,\n",
    "                                    foreigner_masculine, latitude, longitude, index)\n",
    "                    VALUES ({data_to_insert[0]}, {data_to_insert[1]}, {data_to_insert[2]}, {data_to_insert[3]},\n",
    "                            {data_to_insert[4]}, {data_to_insert[5]}, {data_to_insert[6]},\n",
    "                            {data_to_insert[7]}, {data_to_insert[8]}, {data_to_insert[9]});\n",
    "                \"\"\"\n",
    "\n",
    "    cur.execute(insert_sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def truncate_table(schema, table_name):\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    truncate_sql = f\"\"\"\n",
    "                    truncate table {schema}.{table_name}\n",
    "\n",
    "                \"\"\"\n",
    "    cur.execute(truncate_sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e143ef-74fa-4d66-8bd5-aeeeecb892c6",
   "metadata": {},
   "source": [
    "**Part Two:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064c9ce-3053-43b2-85b1-fb228114230f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import Actions\n",
    "schema = Actions.schema_containing_tables                # declare database schema here\n",
    "\n",
    "\n",
    "for i in range(len(Actions.nat_links)):\n",
    "\n",
    "    nat_table_link = Actions.nat_links[i]\n",
    "    pop_table_link = Actions.age_group_links[i]\n",
    "    nat_table_name = Actions.table_names[i][0]            # nationality table names\n",
    "    ageGr_table_name = Actions.table_names[i][1]          # ageGroups table names\n",
    "\n",
    "    # making request to the website\n",
    "    response = requests.get(nat_table_link)                # nationality\n",
    "    response2 = requests.get(pop_table_link)               # age Groups\n",
    "\n",
    "    # getting the web content as text\n",
    "    pop_by_nat_html = response.text             # pop by nationality html from BW website\n",
    "    pop_by_ageGroups = response2.text           # pop by age groups html from BW website\n",
    "\n",
    "    # Using beautiful soup and html parser to pass the response as html\n",
    "    soup = BeautifulSoup(pop_by_nat_html, \"html.parser\")            # For population by nationality\n",
    "    soup2 = BeautifulSoup(pop_by_ageGroups, \"html.parser\")          # For population by age groups\n",
    "\n",
    "    # grabbing the needed table rows from the website contents\n",
    "    pop_by_nat_table_rows = soup.find_all(name=\"tr\")[25:]          # indexing to print only from the desired year\n",
    "    pop_by_ageG_table_rows = soup2.find_all(name=\"tr\")[6:]\n",
    "\n",
    "    # create table if it does not exist\n",
    "    Actions.create_table_age_groups(schema=schema, table_name=ageGr_table_name)\n",
    "\n",
    "    # truncating old data in table to be replaced with new data\n",
    "    Actions.truncate_table(schema=schema, table_name=ageGr_table_name)    # deletes existing records in table\n",
    "\n",
    "    # Write the data rows to the database\n",
    "    year_omit = '1987'  # removing some duplicate years in the data\n",
    "    counts = 0\n",
    "    for row in pop_by_ageG_table_rows:\n",
    "        age_list = []\n",
    "        print(\"=========\")\n",
    "\n",
    "        age_year = row.find(name=\"th\").getText()[:4].replace(\"*\", \"\")\n",
    "        if age_year == year_omit and counts == 0:\n",
    "            counts += 1\n",
    "            continue\n",
    "        total_pop = row.find_all(name=\"td\")[0].getText().replace(\".\", \"\")\n",
    "        under15 = row.find_all(name=\"td\")[1].getText().replace(\".\", \"\")\n",
    "        fifteenToEighteen = row.find_all(name=\"td\")[2].getText().replace(\".\", \"\")\n",
    "        EighteenToTwentyfive = row.find_all(name=\"td\")[3].getText().replace(\".\", \"\")\n",
    "        twentyfiveToforty = row.find_all(name=\"td\")[4].getText().replace(\".\", \"\")\n",
    "        fortyTosixtyfive = row.find_all(name=\"td\")[5].getText().replace(\".\", \"\")\n",
    "        sixtyfiveAndAbove = row.find_all(name=\"td\")[6].getText().replace(\".\", \"\")\n",
    "        latitude = Actions.lat_lon[i][0]\n",
    "        longitude = Actions.lat_lon[i][1]\n",
    "        index = Actions.special_id[i]\n",
    "\n",
    "        age_list.extend(\n",
    "            [age_year, total_pop, under15, fifteenToEighteen, EighteenToTwentyfive, twentyfiveToforty, fortyTosixtyfive,\n",
    "             sixtyfiveAndAbove, latitude, longitude, index])  # latitude, longitude, index\n",
    "        # print(age_list)\n",
    "\n",
    "        Actions.insert_data_a(data_to_insert=age_list, schema=schema, table_name=ageGr_table_name)\n",
    "\n",
    "\n",
    "# next table\n",
    "    # create pop by nat table if it does not exist\n",
    "    Actions.create_table_nats(schema=schema, table_name=nat_table_name)\n",
    "\n",
    "    # truncate existing table\n",
    "    Actions.truncate_table(schema=schema, table_name=nat_table_name)     # deletes existing record in table\n",
    "\n",
    "    # Write the data rows to the database\n",
    "    char = \"*\"\n",
    "    for rows in pop_by_nat_table_rows:\n",
    "        nat_list = []\n",
    "        print(\"=========\")\n",
    "        nat_year = rows.find(name=\"th\").getText()\n",
    "        if char in nat_year:\n",
    "            continue\n",
    "        total_pop = rows.find_all(name=\"td\")[0].getText().replace(\".\", \"\")\n",
    "        total_masculine = rows.find_all(name=\"td\")[1].getText().replace(\".\", \"\")\n",
    "        total_german = rows.find_all(name=\"td\")[2].getText().replace(\".\", \"\")\n",
    "        german_masculine = rows.find_all(name=\"td\")[3].getText().replace(\".\", \"\")\n",
    "        total_foreigners = rows.find_all(name=\"td\")[4].getText().replace(\".\", \"\")\n",
    "        foreigner_masculine = rows.find_all(name=\"td\")[5].getText().replace(\".\", \"\")\n",
    "        latitude = Actions.lat_lon[i][0]\n",
    "        longitude = Actions.lat_lon[i][1]\n",
    "        index = Actions.special_id[i]\n",
    "\n",
    "        nat_list.extend(\n",
    "            [nat_year, total_pop, total_masculine, total_german, german_masculine, total_foreigners,\n",
    "             foreigner_masculine, latitude, longitude, index])\n",
    "\n",
    "        Actions.insert_data_n(data_to_insert=nat_list, schema=schema, table_name=nat_table_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701de812-4fa2-4805-9b0c-e069a1c2a19d",
   "metadata": {},
   "source": [
    "**Part Three:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3d05d-74f8-44b6-8f98-d73726c50a07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This script get the osm data as zip file, unzips it into a folder and writes it to a database.\n",
    "# it makes use of the request, zipfile, geopandas and sqlalchemy libraries.\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "\n",
    "# Parameters\n",
    "osm_site = \"https://download.geofabrik.de/europe/germany/baden-wuerttemberg/stuttgart-regbez-latest-free.shp.zip\"\n",
    "user = \"postgres\"\n",
    "password = \"XXXXX\"\n",
    "host = \"localhost\"\n",
    "port = 5432\n",
    "database = \"geodb\"\n",
    "\n",
    "\n",
    "# using the sqlalchemy create_engine method to create a connection\n",
    "conn = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "engine = create_engine(conn)\n",
    "\n",
    "\n",
    "# Download the OSM data file\n",
    "response = requests.get(osm_site)\n",
    "file_path = \"osm_data.zip\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Extract the OSM zip data to a folder\n",
    "extracted_folder = \"extracted_data\"\n",
    "\n",
    "with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)\n",
    "\n",
    "\n",
    "# defining a function to print all files in a folder. Here I used it to get all files in extracted data folder\n",
    "def print_file_names(folder):\n",
    "    # create empty list to store file names\n",
    "    all_files_names = []\n",
    "    # check first if folder is valid\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"Error: '{folder}' is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    # Getting the list of files in the folder\n",
    "    files_folder = os.listdir(folder)\n",
    "\n",
    "    # Iterating over the files and printing their names\n",
    "    for files in files_folder:\n",
    "        file_path_ = os.path.join(folder, files)\n",
    "        if os.path.isfile(file_path_):\n",
    "            all_files_names.append(file)\n",
    "    return all_files_names\n",
    "\n",
    "\n",
    "my_folder = r\"./extracted_data\"\n",
    "file_names = print_file_names(my_folder)\n",
    "print(file_names)\n",
    "\n",
    "count = 0   # added to enable me print only the desire number of files in folder\n",
    "# getting only the shapefile names among the file.\n",
    "for i in file_names:\n",
    "    # if \".shp\" in i and count < 2:           # I added the count to get just the first two shapefiles\n",
    "    if \".shp\" in i:\n",
    "        shape_file_name = i\n",
    "        # print(shape_file_name)\n",
    "        table_name = shape_file_name.replace(\"_free_1.shp\", \"\")  # I trim the filename to be used as table names in\n",
    "        # the DB\n",
    "\n",
    "    # Using GeoPandas to read shapefile using\n",
    "#    gdf = gpd.read_file(\"./extracted_data/gis_osm_buildings_a_free_1.shp\")   # for writing only one\n",
    "        file_path = os.path.join(my_folder, shape_file_name)\n",
    "        gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # Import the shapefile into the postgresSQL database using SQLAlchemy ORM library\n",
    "        gdf.to_postgis(name=table_name, con=engine, schema=\"hft\")\n",
    "        print(\"success\")\n",
    "        # count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
